# conf.py will parse the yaml and extract parameters based on what is specified
# note, the YAML parser will NOT evaluate expressions in the value fields.
# e.g. "validation_frac: 1.0/3.0" will result in str value "1.0/3.0"

# will read and write (normalization, etc.) shot data
# in fs_path / [username] / signal_data | shot_lists | processed shots, etc.
# (username is automatically added as first subdir if user_subdir==True)

# will output csvlog, trained model checkpoints, etc.
# in fs_path_output / [username] / results | csv_logs | model_checkpoints | Graph, etc.

# switches determined by CLI:
# - distributed
# - TBD: mixed precision


num_gpus_per_node: 4
framework: 'tensorflow'
precision: 'float32'
paths:
  fs_path: '/Users/'
  user_subdir: True
  fs_path_output: '/Users/'
  user_subdir_output: True
  signal_prepath: '/signal_data/'
  shot_list_dir: '/shot_lists/'
  tensorboard_save_path: '/Graph/'
data:
  dataset: d3d_0D
  specific_signals: []
  bleed_in: 0
  bleed_in_repeat_fac: 1
  bleed_in_remove_from_test: True
  bleed_in_equalize_sets: False
  signal_to_augment: None
  augmentation_mode: None
  augment_during_training: False
  cut_shot_ends: True
  recompute: False
  recompute_normalization: False
  current_index: 0
  plotting: False
  max_num_shots: 200000
  positive_example_penalty: 1.0
  dt: 0.001
  T_min_warn: 30
  T_max: 1000.0
  T_warning: 1.024
  current_thresh: 750000
  current_end_thresh: 10000
  window_decay: 2
  window_size: 10
  normalizer: 'var'
  norm_stat_range: 100.0
  equalize_classes: False
model:
  target: 'hinge'
  loss_scale_factor: 1.0
  use_batch_norm: false
  pred_length: 200
  pred_batch_size: 128
  rnn_mem_length: 128
  skip: 1
  rnn_size: 200
  rnn_type: 'LSTM'
  rnn_layers: 2
  num_conv_filters: 128
  size_conv_filters: 3
  num_conv_layers: 3
  pool_size: 2
  dense_size: 128
  extra_dense_input: False
  optimizer: 'adam'
  clipnorm: 10.0
  regularization: 0.001
  dense_regularization: 0.001
  lr: 0.00002
  lr_decay: 0.97
  stateful: True
  return_sequences: True
  dropout_prob: 0.1
  single_replica_warmup_steps: 0
  ignore_initial_timesteps: 100
training:
  as_array_of_shots: True
  shuffle_training: True
  # used iff 1) test & 2) (train U validate) are both sampled from the same distribution/source lists of shots:
  train_frac: 0.75
  validation_frac: 0.3333333333333333
  batch_size: 128 # 256
  max_patch_length: 100000
  num_shots_at_once: 200
  max_num_epochs: 1000
  use_mock_data: False
  batch_generator_warmup_steps: 0
  use_process_generator: False
  num_batches_minimum: 20
  ranking_difficulty_fac: 1.0
  timeline_prof: False
  step_limit: 50
  no_validation: True
callbacks:
  list: ['earlystop']
  metrics: ['val_loss','val_roc','train_loss']
  mode: 'max'
  monitor: 'val_roc'
  patience: 5
  write_grads: False
  monitor_test: True
  monitor_times: [30,70,200,500,1000]
